\chapter{Introduction}
\label{chap:Introdu}
%----------------------------------------------------------------------------
\section{Natural Language Processing}
While computers can be easily programmed to understand structured data, such as tables and spreadsheets, it can be rather challenging for them
to understand human communication. Because there is a vast difference in the magnitude of the unstructured data compared to the structured ones, there is a high demand
for tools, that can deal with raw text. Thats where NLP comes in. It contains high variety of tools, that we have to use, when we need to deal with natural input.

Every day, we come into contact with human communication, we say a lot of words to other people, and they try to interpret them even when the context of the saying 
isn't necesseraily complete. The listeners can use their common knowledge to fill the needed information. We resolve ambiguities, misunderstanding, and can even understand words 
we have never heard before just from the context of the communication.
Even though these tasks are trivial for us, for a computer it can be really hard.

%--------------------------------------------------------------------------------------
% egyszer� multtol jelenig bevezeto
The interest in NLP research began in the 1950s, the early phase was mainly focused on MT (Machine Translation), because after the World War II, people
recognised the importance of the translation from one language to another, and hoped to do it automatically.
However MT is still a very difficult nowadays, so these researches discovered early the main challenges of the syntactic and semantic parsing.
As time passed, researches embraced new areas of NLP as more advanced technology and knowledge became available. Now that we live in a world where
computers and smartphones, collecting data became incredibly easy, as a result, statistical NLP drew attention because these models thrive off big data, but one cannot ignore 
simple rule based methods, which can also be a very powerful method, especially using them as a hybrid model with statistical methods.
%--------------------------------------------------------------------------------------
% r�vid nlp pipeline

Building NLP applications requires many levels of analysis.
The typical pipeline structured as follows:
\begin{itemize}
	\item First we need to tokenize our input text, which means breaking up the text into meaningful elements, especially into words
	\item After we tokenized our text, we need word analysis called \texttt{morphology}, which is concerned with the structure of words.
	\item Part of speech explains how a word is used in a sentence, and tagging is a process marking words to its particular part of speech.
	\item The main task of syntactic parsing is to analyze the grammatical structure of a sentence. Given a set of words, a parser forms units (subjects, verbs, etc..) according to some grammatical formalism.
	There are two main type of syntactic parsers:
	\begin{itemize}
		\item Consituency parsers produce trees, that represents the grammatical structure.
		\item Dependency parsers are the more popular nowadays. They represent the structure of a sentence as a dependency tree.
	\end{itemize}
	\item At this point we have various ways to analyze a text, but without modeling its \texttt{meaning}. Semantic is the study of meaning, and semantic parsing is a task to find a representation and assign it to the text. This task will be the main topic of my work.
\end{itemize}

%-------------------------------------------------------------
\section{Objectives}
The main focus of this study is computational semantics. Our research includes building explicit representations of natural language semantics, because in today's state of the art systems for popular semantics tasks such as measuring semantic similarity or machine comprehension they are rarely present. Virtually all systems
competing at popular challenges (e.g. \cite{Cer:2017,Collados:2017}) rely on word embeddings as the sole representation of word meaning. Recently \cite{Recski:2016c} has presented a method using graphical representations of natural language text that improved over the state of the art on the task of
measuring semantic similarity of pairs of English words. In this paper
we use similar graphs as simple but powerful tools for measuring textual
entailment. Our task includes defining new methods for measuring graph similarities, giving us a tool for building strong baseline methods. Our work was based upon measuring our models through state-of-the art systems on the 2018 Semeval Task \textit{Machine comprehension using commonsense knowledge}.

\section{Results}
We present a novel method for recognizing entailment using semantic
graphs and apply it to the 2018 Semeval task on Machine
Comprehension (MC).
Concept graphs are built automatically from MC texts, questions, and answers,
using the semantic parsing system \texttt{4lang} \cite{Recski:2016d}.
First we present a highly automated process of building concept graphs from raw text, after
a strong baseline method using only these graphs is presented achieving an accuracy score of $68.3\%$,
followed by an enhancement of a state-of-the-art system
\cite{Wang:2018}, where we then proceeded to use the metric underlying our baseline as an
additional feature. Preliminary results suggest that these features
achieve a .5 percentage point improvement over the original system.

\section{References}
The code of our system is available on Github\footnote{\url{https://github.com/adaamko/4lang}}. The code was implemented by the authors of this thesis based upon the 4lang system, which was implemented by the supervisor of this thesis, Gábor Recski.

\section{Structure}
The structure of the thesis is the following:
\begin{itemize}
	\item \textbf{Chapter 1} describes the short history and motivation of the NLP applications, it also gives a short summary about the objectives of the thesis, and our results.
	\item \textbf{Chapter 2} presents a short introduction into computational semantics, and discusses the main approaches.
	\item \textbf{Chapter 3} briefly explains the semantic parsing system \textbf{4lang}, and our process of automating the building of concept graphs, and the baseline approach.
	\item \textbf{Chapter 4} gives an introduction into deep learning, focusing on the NLP tasks.
	\item \textbf{Chapter 5} presents our experiments and results with the state-of-the art system \textbf{Yuanfudao}.
	\item \textbf{Chapter 6} summarizes our contributions and describes our ongoing/future work. It briefly discusses our plans for the follow-up, that was beyond the scope of this work.
\end{itemize}